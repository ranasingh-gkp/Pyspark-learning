{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # get the sparkSession if it exists or else create it\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, label: double]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "\n",
    "#Refer: https://spark.apache.org/docs/latest/ml-features#tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "#Refer: https://spark.apache.org/docs/latest/ml-features.html#countvectorizer\n",
    "cv = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"features\", minDF=2.0)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.0066510916073173445,0.9933489083926826], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.9932186374602783,0.006781362539721725], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[3.060935589437575e-07,0.9999996939064411], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9932186374602783,0.006781362539721725], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-03 17:32:23--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119069 (116K) [text/plain]\n",
      "Saving to: ‘sample_linear_regression_data.txt’\n",
      "\n",
      "sample_linear_regre 100%[===================>] 116.28K   180KB/s    in 0.6s    \n",
      "\n",
      "2021-04-03 17:32:27 (180 KB/s) - ‘sample_linear_regression_data.txt’ saved [119069/119069]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download dataset from github\n",
    "! wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer: https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"./sample_linear_regression_data.txt\")\n",
    "\n",
    "\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(), \n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...| -17.026492264209548| -1.6265106840933026|\n",
      "|(10,[0,1,2,3,4,5,...|  -16.71909683360509|-0.01129960392982...|\n",
      "|(10,[0,1,2,3,4,5,...| -15.375857723312297|  0.9008270143746643|\n",
      "|(10,[0,1,2,3,4,5,...| -13.772441561702871|   3.435609049373433|\n",
      "|(10,[0,1,2,3,4,5,...| -13.039928064104615|  0.3670260850771136|\n",
      "|(10,[0,1,2,3,4,5,...|   -9.42898793151394|   -3.26399994121536|\n",
      "|(10,[0,1,2,3,4,5,...|    -9.2679651250406| -0.1762581278405398|\n",
      "|(10,[0,1,2,3,4,5,...|  -9.173693798406978| -0.2824541263038875|\n",
      "|(10,[0,1,2,3,4,5,...| -7.1500991588127265|   3.087239142258043|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.930603551528371| 0.12389571117374062|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.456944198081549| -0.7275144195427645|\n",
      "|(10,[0,1,2,3,4,5,...| -3.2843694575334834| -0.9048235164747517|\n",
      "|(10,[0,1,2,3,4,5,...|   -1.99891354174786|  0.9588887587748192|\n",
      "|(10,[0,1,2,3,4,5,...| -0.4683784136986876|  0.6261083785799368|\n",
      "|(10,[0,1,2,3,4,5,...|-0.44652227528840105| 0.19068393875752507|\n",
      "|(10,[0,1,2,3,4,5,...| 0.10157453780074743| -0.9062122256799047|\n",
      "|(10,[0,1,2,3,4,5,...|  0.2105613019270259|   1.225604620956131|\n",
      "|(10,[0,1,2,3,4,5,...|  2.1214592666251364|  0.2854396644518767|\n",
      "|(10,[0,1,2,3,4,5,...|  2.8497179990245116|  1.3569268250561075|\n",
      "|(10,[0,1,2,3,4,5,...|   3.980473021620311|  2.5359695420417965|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
