{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.144476\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute value of PI using Monte Carlo Approach\n",
    "# Source: http://spark.apache.org/examples.html\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "if not 'sc' in globals():\n",
    "    sc = pyspark.SparkContext() #create SparkContext if not exist\n",
    "\n",
    "    \n",
    "NUM_SAMPLES = 1000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# parallelize: http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=parallelize\n",
    "# filter: http://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES),10) \\\n",
    "             .filter(inside).count() #create 10 RDD  #implement function \n",
    "\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Word Count using Spark\n",
    "\n",
    "text_file = sc.textFile(\"./sample.txt\") #Can be a hdfs://.. path also\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.saveAsTextFile(\"./count_output1\") # can be a hdfs://... directory/path also\n",
    "\n",
    "# $ cat count_output/*\n",
    "# $ ls -lrt count_output : Multiple part files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('India', 2)\r\n",
      "('Updates:', 1)\r\n",
      "('Abdullah,', 1)\r\n",
      "('tested', 1)\r\n",
      "('positive', 1)\r\n",
      "('Covid-19,', 1)\r\n",
      "('admitted', 1)\r\n",
      "('hospital,', 1)\r\n",
      "('says', 1)\r\n",
      "('son', 1)\r\n",
      "('trade', 1)\r\n",
      "('current', 1)\r\n",
      "('Pakistan', 1)\r\n",
      "('Coronavirus', 1)\r\n",
      "('Live', 1)\r\n",
      "('Farooq', 1)\r\n",
      "('who', 1)\r\n",
      "('for', 1)\r\n",
      "('to', 1)\r\n",
      "('Omar.', 1)\r\n",
      "('No', 1)\r\n",
      "('with', 1)\r\n",
      "('under', 1)\r\n",
      "('circumstances:', 1)\r\n",
      "('PM.', 1)\r\n"
     ]
    }
   ],
   "source": [
    "! cat count_output1/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun error: file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hadoop',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Elephant',\n",
       " 'King!',\n",
       " 'A',\n",
       " 'yellow',\n",
       " 'and',\n",
       " 'elegant',\n",
       " 'thing.',\n",
       " 'He',\n",
       " 'never',\n",
       " 'forgets',\n",
       " 'Useful',\n",
       " 'data,',\n",
       " 'or',\n",
       " 'lets',\n",
       " 'C',\n",
       " 'A',\n",
       " 'wonderful',\n",
       " 'king',\n",
       " 'is',\n",
       " 'Hadoop.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " 'plays',\n",
       " 'well',\n",
       " 'with',\n",
       " 'Sqoop.',\n",
       " 'But',\n",
       " 'what',\n",
       " 'helps',\n",
       " 'him',\n",
       " 'to',\n",
       " 'thrive',\n",
       " 'Are',\n",
       " 'Impala,',\n",
       " 'and',\n",
       " 'Hive,',\n",
       " 'And',\n",
       " 'HDFS',\n",
       " 'in',\n",
       " 'the',\n",
       " 'group.',\n",
       " 'HadoopFile2.txt:',\n",
       " '',\n",
       " 'Hadoop',\n",
       " 'is',\n",
       " 'an',\n",
       " 'elegant',\n",
       " 'fellow.',\n",
       " 'An',\n",
       " 'elephant',\n",
       " 'gentle',\n",
       " 'and',\n",
       " 'mellow.',\n",
       " 'He',\n",
       " 'never',\n",
       " 'gets',\n",
       " 'mad,',\n",
       " 'Or',\n",
       " 'does',\n",
       " 'anything',\n",
       " 'bad,',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'Because,',\n",
       " 'at',\n",
       " 'his',\n",
       " 'core,',\n",
       " 'he',\n",
       " 'is',\n",
       " 'yellow.',\n",
       " 'An',\n",
       " 'extraneous',\n",
       " 'element',\n",
       " 'cling!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FlatMap vs Map\n",
    "o1 = text_file.flatMap(lambda line: line.split(\" \")); #flatmap flattd into one big list\n",
    "o1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hadoop', 'is', 'the', 'Elephant', 'King!'],\n",
       " ['A', 'yellow', 'and', 'elegant', 'thing.'],\n",
       " ['He', 'never', 'forgets'],\n",
       " ['Useful', 'data,', 'or', 'lets'],\n",
       " ['C'],\n",
       " ['A', 'wonderful', 'king', 'is', 'Hadoop.'],\n",
       " ['The', 'elephant', 'plays', 'well', 'with', 'Sqoop.'],\n",
       " ['But', 'what', 'helps', 'him', 'to', 'thrive'],\n",
       " ['Are', 'Impala,', 'and', 'Hive,'],\n",
       " ['And', 'HDFS', 'in', 'the', 'group.'],\n",
       " ['HadoopFile2.txt:'],\n",
       " [''],\n",
       " ['Hadoop', 'is', 'an', 'elegant', 'fellow.'],\n",
       " ['An', 'elephant', 'gentle', 'and', 'mellow.'],\n",
       " ['He', 'never', 'gets', 'mad,'],\n",
       " ['Or', 'does', 'anything', 'bad,'],\n",
       " ['A'],\n",
       " ['A'],\n",
       " ['A'],\n",
       " ['Because,', 'at', 'his', 'core,', 'he', 'is', 'yellow.'],\n",
       " ['An', 'extraneous', 'element', 'cling!']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1a = text_file.map(lambda line: line.split(\" \"));\n",
    "o1a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap: Similar to map, it returns a new RDD by applying a function to each element of the RDD,\n",
    "# but output is flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('Elephant', 1),\n",
       " ('King!', 1),\n",
       " ('A', 1),\n",
       " ('yellow', 1),\n",
       " ('and', 1),\n",
       " ('elegant', 1),\n",
       " ('thing.', 1),\n",
       " ('He', 1),\n",
       " ('never', 1),\n",
       " ('forgets', 1),\n",
       " ('Useful', 1),\n",
       " ('data,', 1),\n",
       " ('or', 1),\n",
       " ('lets', 1),\n",
       " ('C', 1),\n",
       " ('A', 1),\n",
       " ('wonderful', 1),\n",
       " ('king', 1),\n",
       " ('is', 1),\n",
       " ('Hadoop.', 1),\n",
       " ('The', 1),\n",
       " ('elephant', 1),\n",
       " ('plays', 1),\n",
       " ('well', 1),\n",
       " ('with', 1),\n",
       " ('Sqoop.', 1),\n",
       " ('But', 1),\n",
       " ('what', 1),\n",
       " ('helps', 1),\n",
       " ('him', 1),\n",
       " ('to', 1),\n",
       " ('thrive', 1),\n",
       " ('Are', 1),\n",
       " ('Impala,', 1),\n",
       " ('and', 1),\n",
       " ('Hive,', 1),\n",
       " ('And', 1),\n",
       " ('HDFS', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('group.', 1),\n",
       " ('HadoopFile2.txt:', 1),\n",
       " ('', 1),\n",
       " ('Hadoop', 1),\n",
       " ('is', 1),\n",
       " ('an', 1),\n",
       " ('elegant', 1),\n",
       " ('fellow.', 1),\n",
       " ('An', 1),\n",
       " ('elephant', 1),\n",
       " ('gentle', 1),\n",
       " ('and', 1),\n",
       " ('mellow.', 1),\n",
       " ('He', 1),\n",
       " ('never', 1),\n",
       " ('gets', 1),\n",
       " ('mad,', 1),\n",
       " ('Or', 1),\n",
       " ('does', 1),\n",
       " ('anything', 1),\n",
       " ('bad,', 1),\n",
       " ('A', 1),\n",
       " ('A', 1),\n",
       " ('A', 1),\n",
       " ('Because,', 1),\n",
       " ('at', 1),\n",
       " ('his', 1),\n",
       " ('core,', 1),\n",
       " ('he', 1),\n",
       " ('is', 1),\n",
       " ('yellow.', 1),\n",
       " ('An', 1),\n",
       " ('extraneous', 1),\n",
       " ('element', 1),\n",
       " ('cling!', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2 = o1.map(lambda word: (word, 1))\n",
    "o2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 4),\n",
       " ('yellow', 1),\n",
       " ('elegant', 2),\n",
       " ('thing.', 1),\n",
       " ('never', 2),\n",
       " ('forgets', 1),\n",
       " ('Useful', 1),\n",
       " ('C', 1),\n",
       " ('wonderful', 1),\n",
       " ('king', 1),\n",
       " ('Hadoop.', 1),\n",
       " ('The', 1),\n",
       " ('plays', 1),\n",
       " ('But', 1),\n",
       " ('Impala,', 1),\n",
       " ('Hive,', 1),\n",
       " ('And', 1),\n",
       " ('HDFS', 1),\n",
       " ('in', 1),\n",
       " ('group.', 1),\n",
       " ('', 1),\n",
       " ('an', 1),\n",
       " ('fellow.', 1),\n",
       " ('gentle', 1),\n",
       " ('mellow.', 1),\n",
       " ('Or', 1),\n",
       " ('anything', 1),\n",
       " ('at', 1),\n",
       " ('his', 1),\n",
       " ('he', 1),\n",
       " ('Hadoop', 2),\n",
       " ('the', 2),\n",
       " ('Elephant', 1),\n",
       " ('King!', 1),\n",
       " ('A', 5),\n",
       " ('and', 3),\n",
       " ('He', 2),\n",
       " ('data,', 1),\n",
       " ('or', 1),\n",
       " ('lets', 1),\n",
       " ('elephant', 2),\n",
       " ('well', 1),\n",
       " ('with', 1),\n",
       " ('Sqoop.', 1),\n",
       " ('what', 1),\n",
       " ('helps', 1),\n",
       " ('him', 1),\n",
       " ('to', 1),\n",
       " ('thrive', 1),\n",
       " ('Are', 1),\n",
       " ('HadoopFile2.txt:', 1),\n",
       " ('An', 2),\n",
       " ('gets', 1),\n",
       " ('mad,', 1),\n",
       " ('does', 1),\n",
       " ('bad,', 1),\n",
       " ('Because,', 1),\n",
       " ('core,', 1),\n",
       " ('yellow.', 1),\n",
       " ('extraneous', 1),\n",
       " ('element', 1),\n",
       " ('cling!', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3 = o2.reduceByKey(lambda a, b: a + b)\n",
    "o3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Spark DataFrame API\n",
    "# Refer:https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # get the sparkSession if it exists or else create it\n",
    "\n",
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"./people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Sources: https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: people1; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [people1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-41f80752a61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM people1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msqlDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: people1; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [people1]\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people1\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[0,3],[1.0,-2.0])\n",
      "[1.0,0.0,0.0,-2.0]\n"
     ]
    }
   ],
   "source": [
    "#4. Vectors and Correaltion Coefficient\n",
    "# https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.linalg.Vectors-class.html\n",
    "s1 = Vectors.sparse(4, [(0, 1.0), (3, -2.0)] )\n",
    "print(s1)\n",
    "print(Vectors.dense(s1))\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(pearson(features)=DenseMatrix(4, 4, [1.0, 0.0556, nan, 0.4005, 0.0556, 1.0, nan, 0.9136, nan, nan, 1.0, nan, 0.4005, 0.9136, nan, 1.0], False))\n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
